<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Mike Dewar</title>
<link href="stylesheet.css" type="text/css" rel="stylesheet" />
</head>
<body>
<div>
<h2 id="leanpub-auto-mike-dewar">Mike Dewar</h2>

<h5 id="leanpub-auto-data-scientist-at-the-new-york-times-rd-lab">Data Scientist at The New York Times R&amp;D Lab</h5>

<p>
  <em>Data Science in Journalism</em>
</p>

<p>
  <em>Mike Dewar is a Data Scientist at the New York Times R&amp;D Lab. Mike holds a PhD from the University of Sheffield, UK, where he studied the modelling of complex systems using data. His current work now focuses on building tools to study behaviour.</em>
</p>

<p>
  <em>Before joining The New York Times, Mike worked at the New York tech company bit.ly, and completed postdoctoral positions at Sheffield, Edinburgh and Columbia Universities. In this interview, you’ll read Mike’s stories about fruit fly necrophilia, how The New York Times looks into the future and ways that data science is affecting journalism.</em>
</p>

<p>
  <em>Mike is a data ambassador for the non-profit organization DataKind, and has published widely on signal processing, machine learning and data visualization.</em>
</p>

<h5 id="leanpub-auto-can-you-trace-your-career-path-for-our-readers-what-got-you-interested-in-data-science-what-got-you-interested-in-bitly-and-the-new-york-times-and-what-projects-have-you-done-that-you-can-share-with-our-readers">Can you trace your career path for our readers? What got you interested in data science? What got you interested in bitly and The New York Times, and what projects have you done that you can share with our readers?</h5>

<p>I got my PhD in Modelling Complex Systems from the University of Sheffield in the UK. The department is called Automatic Control and Systems Engineering, which in the US is sometimes called Controls or Cybernetics - it’s the study of feedback, modelling, and control.</p>

<p>My PhD looked at modelling spatial-temporal systems. The idea is that you would collect data from the physical space and then build dynamic models of how the system evolved through time using the data you collected.</p>

<p>Then I did a few postdoctoral positions. I did a post-doc at the University of Sheffield for a year. We worked with Unilever and I looked at modelling how people were brushing their teeth. By attaching sensors to a toothbrush with accelerometers and positional sensing, they collected all this data about how people brushed their teeth – it was a very strange gig.</p>

<p>I did that for a year, spent some time writing up the papers for the PhD, and then I decamped to Edinburgh University, where I worked in the School of Informatics, studying the behaviour of fruit flies. The biologists would alter the brain of the fruit fly and observe their changes in behaviour. In courtship behavior, specifically, the changes were easy to see. If you place a male fruit fly in a small space with a female fruit fly, even the dead body of a female fruit fly, it will mate with her. Well, it will definitely try at least, which is a bit grim.</p>

<p>So, there were loads of fun modelling of sequences and some nice machine learning. I even got to learn how to prepare mutant fruit flies. Most of this work was done at Edinburgh but also included a little bit of work at Harvard, at the Longwood Campus. Then I got the gig at Columbia, which was in the Applied Physics and Applied Math Department. That was with Professor Chris Wiggins, who you might have come across in your studies of data science.</p>

<p>He and Hilary Mason wrote a blog post which outlined various steps of data science, namely: “Obtain, scrub, explore, model, interpret.” The steps outlined this idea of a data science flow being practical and producing tangible outputs. Chris was thinking a lot about that with Hilary while I was studying T-cells.</p>

<p>There are lots of different types of T-cells - the population of these different T-cells in your body changes before, during and after an infection (this is how immunization works). So after an infection, you have “memory” T-cells in your body. The group at Columbia was very interested in how T-cells change to this “memory” state.</p>

<p>They collected lots of genetic data and looked for different genes that were responsible for changing the state of these populations of cells. You’d be working with 8 microarrays, but each microarray would have 25,000 genes on it. You had a very strange machine learning problem, whereby you had very little data to go on but it felt like you had a lot because of all the features.</p>

<p>It was through Chris Wiggins that I met Hilary Mason, who was my boss at Bitly. I had also become engaged to a girl who lives in New York, so when it came time to start thinking about what was next after my post doctorate at Columbia, it was important to me to stay in New York. But life as a postdoctoral student in New York sucks because it’s quite expensive here. At the same time, the idea of “big data” was just coming to the forefront. There were numerous social media companies that were just starting to think about what they might do with all their data. I was interested in behaviour and making tools for studying behaviour, so Hilary showed up at just the right moment when I wanted to pay the rent, stay in New York, study behaviour, and use lots of data while doing it.</p>

<p>So I jumped ship and went to Bitly as a data scientist. I think I’m probably amongst the first people who had that title. I made tools at Bitly for studying very large numbers of people’s behaviour and trying to build interesting, potentially profitable streams.</p>

<p>Bitly ran its course. I was there for about a year and a half. We did lots of interesting things, but it became time to move on. About that time, a position at The New York Times R&amp;D Lab showed up, which was somewhere I’d wanted to work for years, so I moved over to the lab where I’ve been now for about two years doing all sorts of interesting things.</p>

<p>Essentially, leaving academia is a moment where you have to decide if you want to be a professor or not, and I think I’d already decided that was not quite what I wanted to do. I like coding and making things, but I don’t enjoy talking all day, so that was the decision I made.</p>

<h5 id="leanpub-auto-weve-been-talking-to-a-lot-of-people-who-decided-to-jump-ship-from-academia-it-seems-like-a-lot-of-them-have-been-citing-reasons-such-as-the-lack-of-dynamism-they-felt-that-data-science-was-much-more-interesting-and-fast-paced-did-you-feel-that-as-well">We’ve been talking to a lot of people who decided to jump ship from academia. It seems like a lot of them have been citing reasons such as the lack of dynamism. They felt that data science was much more interesting and fast paced. Did you feel that as well?</h5>

<p>No, not really. Academia was very fast paced and very intense, with cutting edge research. The stuff I got to work on was amazing. Watching the very modern imaging of T-cells changing and learning about viruses was overwhelmingly fascinating. When the practical wasn’t going quickly, the theoretical was going quickly, and there were always ten different things to do.</p>

<p>I had a very interesting time in academia. Postdoctoral positions are great fun. Lecturing, however, didn’t look like so much fun. I wanted to hold onto the fun bits of academia and get paid, which is no small thing when you are starting a family. When staying in New York, which is a bizarrely expensive place, a certain set of constraints comes your way. In short, academia was amazing.</p>

<h5 id="leanpub-auto-it-seems-like-from-your-academic-background-you-learned-a-lot-from-looking-at-a-complex-system-a-mass-of-data-and-extracting-stories-and-hypotheses-from-that-you-talk-about-how-the-unifying-theme-in-data-science-is-actually-just-identifying-massive-behavioural-phenomena-what-is-your-advice-for-identifying-the-questions-telling-the-stories-and-identifying-the-hypotheses-in-the-data-set-especially-since-you-say-data-science-is-all-about-abductive-reasoning-finding-stories-and-learning-from-data-what-is-your-advice-for-learning-which-story-to-tell-with-data-and-what-to-look-at">It seems like from your academic background you learned a lot from looking at a complex system, a mass of data, and extracting stories and hypotheses from that. You talk about how the unifying theme in data science is actually just identifying massive behavioural phenomena. What is your advice for identifying the questions, telling the stories, and identifying the hypotheses in the data set; especially since you say data science is all about abductive reasoning, finding stories, and learning from data? What is your advice for learning which story to tell with data and what to look at?</h5>

<p>The key piece of advice is always to draw lots of pictures and draw them very quickly. Draw pictures of how things work, even just flow diagrams or engineering block diagrams. Make very rough, quick visualizations of what’s in the data, starting with time series and histograms. Thinking hard about graphical modelling and really trying to get to grips with the system and data set that’s in front of you helps you think about how the probabilities fit together.</p>

<p>The danger that I see people getting into is that the drawing of the picture becomes the last thing you do, like when you’re reading an academic paper. The results and pictures are always at the end of an academic paper, which is a terrible shame. I think the paper should start with pictures of time series and distributions, and go from there into the theory. That’s often how we work.</p>

<p>That would be my very general advice: to fail early and to fail often. It’s okay to draw lots and lots of pictures that might all be rubbish, but if you draw pictures quickly and really start to understand what’s actually going on, you begin to get much deeper ideas of what the right questions are, than if you just start with a classifier.</p>

<h5 id="leanpub-auto-can-you-elaborate-on-drawing-pictures-a-bit-more">Can you elaborate on drawing pictures a bit more?</h5>

<p>I learned a lot at Edinburgh about graphical modelling, which is a very simple technique for exploring conditional probabilities and trying to explore how random variables in a system affect one another. The beautiful thing about graphical models is that if you start drawing them, you are, at the very same time, beginning to explain your assumptions about the system. Also, you’re starting to do quite a mathematical task of imposing some structure that you can then test. I really enjoy quickly trying to show whoever I’m working with a graphical model of how I think things work. The conversation gets going very quickly and it leads to testable hypotheses, which is great.</p>

<p>The other interpretation of drawing things quickly is to get immediately into the data set. As soon as someone hands you a data set or gives you access to a stream, the very first thing to do is to find an interesting variable in the data set and plot it. If it’s over time, plot a time series. If you’ve got lots of samples of that variable then plot a distribution. If it’s both then plot both. You can do that using Python, or R, or Tableau, or Excel. Do that first and don’t waste time. It takes five minutes to make some plots.</p>

<p>The reason is that it gets you thinking about your assumptions in the same way that graphical models do. The distributions and the time series get you thinking about the data. Both of those together are the beginning of a modelling process that will see you in good stead. It’s quite an iterative process. If all you’ve got is a Bash terminal, then I would sort my data and then pipe that to “uniq –c” to get a really cheap histogram.</p>

<h5 id="leanpub-auto-you-say-that-visualization-and-communicating-data-is-very-important-because-it-helps-other-people-generate-hypotheses-and-trust-the-data-what-advice-do-you-have-for-the-best-way-to-approach-making-visualizations-to-an-internal-company-audience">You say that visualization and communicating data is very important because it helps other people generate hypotheses and trust the data. What advice do you have for the best way to approach making visualizations to an internal company audience?</h5>

<p>One thing we’ve been doing lately is trying hard to show all the data. I would normally start by thinking about how a system is working and what I’m trying to get out of a data set. Then I would draw some aggregate visualisations, for example, a histogram if I’m interested in how things are distributed, or a line plot if I’m interested in a time series.</p>

<p>One thing we’ve tried to do more recently is to draw every single data point in a visualisation, rather than aggregating, just like in a scatter plot. This is something made much easier as I now get to work regularly with Nik Hanselmann, who is a creative technologist in the lab and is extremely adept at this sort of thing. If you can make a scatter plot of a large data set interpretable, then that act of showing all of the data points allows people to see a zoomed out view of the whole thing and allows them to pick on individual data points. They see the outliers and wonder why there’s an outlier there.</p>

<p>Clusters are another good example. If you’ve done your scatter plot well and people can start to pick out different features of the scatter plot by looking directly at the bit of data that you want to show them, then they start to ask questions and start to wonder. That helps you as the analyst or the data scientist. It helps you in trying to understand what your audience is actually interested in and how you might help them make decisions. It’s an incredibly difficult thing to do without some sort of interaction like that. Trying to show all the data points is quite challenging sometimes, but that’s been oddly effective over the last year or so.</p>

<p>Other than that, axis labels. I feel old saying it but lots of people don’t put axis labels on things. You read lots of blog posts about lying with statistics and all that sort of stuff and all the tricks people play, which is fine, but it’s very difficult to get away with those tricks if you label your axes properly. You shouldn’t trust any graph that doesn’t have their axis labelled properly.</p>

<h5 id="leanpub-auto-how-do-you-think-this-whole-explosion-of-data-as-well-as-computational-power-and-analysis-on-top-of-that-is-going-to-affect-the-nature-of-journalism">How do you think this whole explosion of data, as well as computational power and analysis on top of that, is going to affect the nature of journalism?</h5>

<p>There are a few parts to your question. There have been computer-assisted reporting (CAR) journalists for a long time now. Our computer-assisted reporting desk has been around for many years so the idea that data has been affecting journalism is not a new one.</p>

<p>This is how I came to grips with the term “big data.” A friend of mine pointed out that we should think about big data like we think about punk – a cultural moment that was meaningfully hyped for a period, which then led to a lasting change in society.</p>

<p>I like this idea of “big data” as a cultural moment because there’s been a definite change in the amount of data we can collect and the expectation of collecting data in the first place. The standard costs of storage, processing, and transmission have all gone down. There has been, over the last few years, a dramatic cultural shift in and around data storage. That hasn’t gone by journalists - it’s quite the opposite. What they’re faced with are huge data sets that they think might contain stories. Or it’ll be the other way round where they believe that there is a story and will use the FOIA (Freedom of Information Act) to get data sets.</p>

<p>When they’re telling a story, or they believe that there’s data associated with the story, they will search government organizations or FOIA organizations that have worked with the government and are subject to the Freedom of Information Act. I think the rise of the FOIA is an interesting response to big data in the sense that a journalist will often assume that there is data associated with a story and will demand to see it.</p>

<p>Rather than the WikiLeaks style, where there is a huge data dump for one reason or another, journalists will believe that there is data associated with their story and will use FOIA data in order to support the story. This is a lot more work, and that work is a lot more laborious. The impact of big data is a culture where we expect there to be data.</p>

<p>The other side of that, which I think is probably a bit sexier, is the WikiLeaks side of things, where there is a huge pile of data that is being made available - like the Medicare data. There’s a huge data set that’s been released around how Medicare dollars are spent with personal information about doctors that receive Medicare funding. There have been a lot of stories out of that data set that are very interesting. That’s the other mode that journalism works in. That’s when people want to use R, or Python, to clean and analyse the data and d3, ggplot, or matplotlib to build visualizations of that data set. D3 is especially interesting because it’s used to make web and print graphics, which is why you see it a lot.</p>

<h5 id="leanpub-auto-what-can-you-share-with-us-about-what-you-do-at-the-rd-lab-especially-since-most-of-the-people-weve-been-talking-to-work-at-technology-companies-instead-of-a-journalism-company-with-a-very-strong-technology-component">What can you share with us about what you do at the R&amp;D Lab, especially since most of the people we’ve been talking to work at technology companies, instead of a journalism company with a very strong technology component?</h5>

<p>The R&amp;D Lab was set up in 2006 to fulfill a number of roles. Specifically, it tries to think three to five years into the future, tracking social, cultural and technological trends relevant to The NYT. That gives us quite a range of possible projects.</p>

<p>The other function of the R&amp;D Lab is essentially to listen. That takes two forms. One is a futurist approach where we try and watch what’s going on in the blogosphere and watch what’s going on in new technologies. We try and keep an ear out for anything that looks like it might have something to do with the future.</p>

<p>We also act as a gateway. If someone is developing a new, interesting business software  that they think The New York Times might be interested in, but there’s not an immediately clear use case, often we’ll speak to them. We’ll ask them some questions and try and understand what they think the future looks like. We can think about how that fits in with how The New York Times thinks about the future.</p>

<p>In terms of projects, it’s quite varied. We’re thinking a lot lately about how to extract information from article data. Given an article, can you extract all of the statistics, the quotes, facts and events? This is a tired old problem, so we’re trying to think about other ways we might accomplish that. Can we capture information like that during the writing process or the editing process or the production process, rather than approaching the articles with an extractive, natural language processing view? It would be much more interesting to see what metadata we could generate in the first place.</p>

<p>That’s an example of journalistic stuff. Then, we think about how the news might be presented in the future. One example is a good idea that the lab had regarding the future of tablets. The New York Times R&amp;D Lab had thought about what a tablet reader application would be like well before the iPad came out. When the iPad did come out, The New York Times had a head start in understanding how people might interact with their tablet and what would be interesting to show on it.</p>

<h5 id="leanpub-auto-what-advice-do-you-have-for-other-phd-students-and-people-in-academia-transitioning-to-data-science-especially-since-youve-been-through-this-already-what-advice-would-you-give-someone-interested-in-transitioning-to-data-science">What advice do you have for other PhD students and people in academia transitioning to data science, especially since you’ve been through this already? What advice would you give someone interested in transitioning to data science?</h5>

<p>Code in public, that’s number one. If you’re going to be a data scientist, you’re probably going to have to be able to program in one way or another. There are lots of different options, but you’re probably going to have to be quantitative and be able to write non-trivial programs on the computer. As you code, as you practice, as you go to hackathons, as you code for your post doctorate or for your PhD or for your graduate degree, make sure you do it in public. Put it on Github. To a certain extent I’m on the other side of it now where I put every thing I think of on Github, so it’s a bit of a mess.</p>

<p>Especially with PhDs, one of the problems we see is that although they come from impressive universities, they have impressive resumes, and they’ve written these nice papers, but we still have no idea if they can actually write code. That makes them more difficult to hire.</p>

<p>Coding in public also encourages you to engage with communities that you work with. There are programming communities that share your languages; academic communities that might want to use your code to test out your claims; and companies that want to evaluate you and reduce the risk in hiring you.</p>

<p>The other thing is networking. It’s more or less the same thing, but it’s important. In major cities it’s very easy for you to get out of your office or house and visit meetups and user groups to give a talk. Giving talks about your academic work to lay people is an incredibly interesting and enlightening experience, one that you should go through. It also exposes you to the business communities and the various kinds of people that you might want to get jobs from in the future. It also shows you what other people are up to; it knocks your academic naivety very quickly, which is great.</p>

<p>Other than coding in public and networking, try to apply all your work to something. I wrote three papers for my PhD, and they were all about the EM algorithm. There’s a load of spatial-temporal models that I put a lot of work in to. Over 3-4 years, I wrote some papers, and nobody cared, nobody at all. However, when we applied this theory to modelling troop movements in Afghanistan, lots of people cared. We won awards. We wrote a book. We were in the news. The idea of taking the advanced things that you learn at school and applying them to something important and meaningful exposes you to a world that’s difficult to see from the incremental science of being a good student.</p>
</div>
</body>
</html>
